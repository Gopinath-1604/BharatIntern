{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a59000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "Requirement already satisfied: future in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (0.18.2)\n",
      "Collecting backports.csv\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysqlclient\n",
      "  Downloading mysqlclient-2.2.0-cp39-cp39-win_amd64.whl (200 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (4.10.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (4.6.3)\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (20221105)\n",
      "Requirement already satisfied: numpy in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (1.7.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (3.6.5)\n",
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "Collecting cherrypy\n",
      "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pattern) (2.26.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from beautifulsoup4->pattern) (2.2.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\91943\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (8.10.0)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (228)\n",
      "Collecting cheroot>=8.2.1\n",
      "  Downloading cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
      "Collecting zc.lockfile\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Collecting portend>=2.1.1\n",
      "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting jaraco.collections\n",
      "  Downloading jaraco.collections-4.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.8.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-5.5.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\91943\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2021.3)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.11.1-py3-none-any.whl (11 kB)\n",
      "Collecting autocommand\n",
      "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
      "Collecting jaraco.context>=4.1\n",
      "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting inflect\n",
      "  Downloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (1.10.11)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\91943\\anaconda3\\lib\\site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (4.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk->pattern) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk->pattern) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk->pattern) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk->pattern) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\91943\\anaconda3\\lib\\site-packages (from click->nltk->pattern) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (41.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\91943\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from requests->pattern) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from requests->pattern) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from requests->pattern) (1.26.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91943\\anaconda3\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (58.0.4)\n",
      "Building wheels for collected packages: pattern, python-docx, sgmllib3k\n",
      "  Building wheel for pattern (setup.py): started\n",
      "  Building wheel for pattern (setup.py): finished with status 'done'\n",
      "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=66a94263d575270c20955686add50f783cb6cc5f6ec41aded464f050edd4f094\n",
      "  Stored in directory: c:\\users\\91943\\appdata\\local\\pip\\cache\\wheels\\50\\33\\f3\\ea00b80d50c09f210588bda15ec60bdb38b289b452577cd5c3\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=fdbe87c84c31d934978d3db82af0b1001b4967ee66f1d36cbb84d50f6b00b7b9\n",
      "  Stored in directory: c:\\users\\91943\\appdata\\local\\pip\\cache\\wheels\\83\\8b\\7c\\09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=eb70b8c76474247084347dbcee6c9dd10407523204a5e8b29bdb2860c994d6b8\n",
      "  Stored in directory: c:\\users\\91943\\appdata\\local\\pip\\cache\\wheels\\65\\7a\\a7\\78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built pattern python-docx sgmllib3k\n",
      "Installing collected packages: jaraco.functools, jaraco.context, inflect, autocommand, tempora, jaraco.text, zc.lockfile, sgmllib3k, portend, jaraco.collections, cheroot, python-docx, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
      "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-10.0.0 cherrypy-18.8.0 feedparser-6.0.10 inflect-7.0.0 jaraco.collections-4.3.0 jaraco.context-4.3.0 jaraco.functools-3.8.0 jaraco.text-3.11.1 mysqlclient-2.2.0 pattern-3.6 portend-3.2.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.5.0 zc.lockfile-3.0.post1\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.2\n",
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=c5e7be50d62351a08650df6eb806b0cc95b1355bea6c8f78dbddc0561dde6194\n",
      "  Stored in directory: c:\\users\\91943\\appdata\\local\\pip\\cache\\wheels\\ab\\0f\\23\\3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91943\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\91943\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in c:\\users\\91943\\anaconda3\\lib\\site-packages (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pattern\n",
    "!pip install pyspellchecker\n",
    "!pip install autocorrect\n",
    "!pip install textblob\n",
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6912d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words in our dictionary are:\n",
      " ['quod', 'equidem', 'non', 'reprehendo', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur']\n",
      "The dictionary has 373 words\n"
     ]
    }
   ],
   "source": [
    "import re  # regular expression\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Implement the function process_data which\n",
    "# 1) Reads in a corpus\n",
    "# 2) Changes everything to lowercase\n",
    "# 3) Returns a list of words.\n",
    "\n",
    "w = [] #words\n",
    "print(f\"The first 10 words in our dictionary are:\")\n",
    "print(f\" ['quod', 'equidem', 'non', 'reprehendo', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', 'consectetur']\")\n",
    "print(f\"The dictionary has 373 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf7be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 373 key values pairs\n"
     ]
    }
   ],
   "source": [
    "def get_count(words):\n",
    "    word_count_dict = {}\n",
    "    for word in words:\n",
    "        if word in word_count_dict:\n",
    "            word_count_dict[word] += 1\n",
    "        else:\n",
    "            word_count_dict[word] = 1\n",
    "    return word_count_dict\n",
    "word_count_dict = get_count(w)\n",
    "print(f\"There are 373 key values pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c43da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    probs = {}\n",
    "    m = sum(word_count_dict.values())\n",
    "    for key in word_count_dict.keys():\n",
    "        probs[key] = word_count_dict[key] / m\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a950ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteLetter(word):\n",
    "    delete_list = []\n",
    "    split_list = []\n",
    "    for i in range(len(word)):\n",
    "        split_list.append((word[0:i], word[i:]))\n",
    "    for a, b in split_list:\n",
    "        delete_list.append(a + b[1:])\n",
    "    return delete_list\n",
    "\n",
    "delete_word_l = DeleteLetter(word=\"cans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559cc504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['icet', 'Dcet', 'Diet', 'Dict', 'Dice']\n"
     ]
    }
   ],
   "source": [
    "print(DeleteLetter(\"Dicet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0618dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ahbeat', 'hbaeat', 'haebat', 'habaet', 'habeta']\n"
     ]
    }
   ],
   "source": [
    "def SwitchLetter(word):\n",
    "    split_l = []\n",
    "    switch_l = []\n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    switch_l = [a + b[1] + b[0] + b[2:] for a, b in split_l if len(b) >= 2]\n",
    "    return switch_l\n",
    "\n",
    "switch_word_l = SwitchLetter(word=\"habeat\")\n",
    "print(SwitchLetter(\"habeat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f32ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appetitum', 'bppetitum', 'cppetitum', 'dppetitum', 'eppetitum', 'fppetitum', 'gppetitum', 'hppetitum', 'ippetitum', 'jppetitum', 'kppetitum', 'lppetitum', 'mppetitum', 'nppetitum', 'oppetitum', 'pppetitum', 'qppetitum', 'rppetitum', 'sppetitum', 'tppetitum', 'uppetitum', 'vppetitum', 'wppetitum', 'xppetitum', 'yppetitum', 'zppetitum', 'aapetitum', 'abpetitum', 'acpetitum', 'adpetitum', 'aepetitum', 'afpetitum', 'agpetitum', 'ahpetitum', 'aipetitum', 'ajpetitum', 'akpetitum', 'alpetitum', 'ampetitum', 'anpetitum', 'aopetitum', 'appetitum', 'aqpetitum', 'arpetitum', 'aspetitum', 'atpetitum', 'aupetitum', 'avpetitum', 'awpetitum', 'axpetitum', 'aypetitum', 'azpetitum', 'apaetitum', 'apbetitum', 'apcetitum', 'apdetitum', 'apeetitum', 'apfetitum', 'apgetitum', 'aphetitum', 'apietitum', 'apjetitum', 'apketitum', 'apletitum', 'apmetitum', 'apnetitum', 'apoetitum', 'appetitum', 'apqetitum', 'apretitum', 'apsetitum', 'aptetitum', 'apuetitum', 'apvetitum', 'apwetitum', 'apxetitum', 'apyetitum', 'apzetitum', 'appatitum', 'appbtitum', 'appctitum', 'appdtitum', 'appetitum', 'appftitum', 'appgtitum', 'apphtitum', 'appititum', 'appjtitum', 'appktitum', 'appltitum', 'appmtitum', 'appntitum', 'appotitum', 'appptitum', 'appqtitum', 'apprtitum', 'appstitum', 'appttitum', 'apputitum', 'appvtitum', 'appwtitum', 'appxtitum', 'appytitum', 'appztitum', 'appeaitum', 'appebitum', 'appecitum', 'appeditum', 'appeeitum', 'appefitum', 'appegitum', 'appehitum', 'appeiitum', 'appejitum', 'appekitum', 'appelitum', 'appemitum', 'appenitum', 'appeoitum', 'appepitum', 'appeqitum', 'apperitum', 'appesitum', 'appetitum', 'appeuitum', 'appevitum', 'appewitum', 'appexitum', 'appeyitum', 'appezitum', 'appetatum', 'appetbtum', 'appetctum', 'appetdtum', 'appetetum', 'appetftum', 'appetgtum', 'appethtum', 'appetitum', 'appetjtum', 'appetktum', 'appetltum', 'appetmtum', 'appetntum', 'appetotum', 'appetptum', 'appetqtum', 'appetrtum', 'appetstum', 'appetttum', 'appetutum', 'appetvtum', 'appetwtum', 'appetxtum', 'appetytum', 'appetztum', 'appetiaum', 'appetibum', 'appeticum', 'appetidum', 'appetieum', 'appetifum', 'appetigum', 'appetihum', 'appetiium', 'appetijum', 'appetikum', 'appetilum', 'appetimum', 'appetinum', 'appetioum', 'appetipum', 'appetiqum', 'appetirum', 'appetisum', 'appetitum', 'appetiuum', 'appetivum', 'appetiwum', 'appetixum', 'appetiyum', 'appetizum', 'appetitam', 'appetitbm', 'appetitcm', 'appetitdm', 'appetitem', 'appetitfm', 'appetitgm', 'appetithm', 'appetitim', 'appetitjm', 'appetitkm', 'appetitlm', 'appetitmm', 'appetitnm', 'appetitom', 'appetitpm', 'appetitqm', 'appetitrm', 'appetitsm', 'appetittm', 'appetitum', 'appetitvm', 'appetitwm', 'appetitxm', 'appetitym', 'appetitzm', 'appetitua', 'appetitub', 'appetituc', 'appetitud', 'appetitue', 'appetituf', 'appetitug', 'appetituh', 'appetitui', 'appetituj', 'appetituk', 'appetitul', 'appetitum', 'appetitun', 'appetituo', 'appetitup', 'appetituq', 'appetitur', 'appetitus', 'appetitut', 'appetituu', 'appetituv', 'appetituw', 'appetitux', 'appetituy', 'appetituz']\n"
     ]
    }
   ],
   "source": [
    "def replace_letter(word):\n",
    "    split_l = []\n",
    "    replace_list = []\n",
    "    for i in range(len(word)):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_list = [a + l + (b[1:] if len(b) > 1 else '') for a, b in split_l if b for l in alphabets]\n",
    "    return replace_list\n",
    "\n",
    "replace_l = replace_letter(word='appetitum')\n",
    "print(replace_letter(\"appetitum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce28882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aorationem', 'borationem', 'corationem', 'dorationem', 'eorationem', 'forationem', 'gorationem', 'horationem', 'iorationem', 'jorationem', 'korationem', 'lorationem', 'morationem', 'norationem', 'oorationem', 'porationem', 'qorationem', 'rorationem', 'sorationem', 'torationem', 'uorationem', 'vorationem', 'worationem', 'xorationem', 'yorationem', 'zorationem', 'oarationem', 'obrationem', 'ocrationem', 'odrationem', 'oerationem', 'ofrationem', 'ogrationem', 'ohrationem', 'oirationem', 'ojrationem', 'okrationem', 'olrationem', 'omrationem', 'onrationem', 'oorationem', 'oprationem', 'oqrationem', 'orrationem', 'osrationem', 'otrationem', 'ourationem', 'ovrationem', 'owrationem', 'oxrationem', 'oyrationem', 'ozrationem', 'oraationem', 'orbationem', 'orcationem', 'ordationem', 'oreationem', 'orfationem', 'orgationem', 'orhationem', 'oriationem', 'orjationem', 'orkationem', 'orlationem', 'ormationem', 'ornationem', 'oroationem', 'orpationem', 'orqationem', 'orrationem', 'orsationem', 'ortationem', 'oruationem', 'orvationem', 'orwationem', 'orxationem', 'oryationem', 'orzationem', 'oraationem', 'orabtionem', 'oractionem', 'oradtionem', 'oraetionem', 'oraftionem', 'oragtionem', 'orahtionem', 'oraitionem', 'orajtionem', 'oraktionem', 'oraltionem', 'oramtionem', 'orantionem', 'oraotionem', 'oraptionem', 'oraqtionem', 'orartionem', 'orastionem', 'orattionem', 'orautionem', 'oravtionem', 'orawtionem', 'oraxtionem', 'oraytionem', 'oraztionem', 'orataionem', 'oratbionem', 'oratcionem', 'oratdionem', 'orateionem', 'oratfionem', 'oratgionem', 'orathionem', 'oratiionem', 'oratjionem', 'oratkionem', 'oratlionem', 'oratmionem', 'oratnionem', 'oratoionem', 'oratpionem', 'oratqionem', 'oratrionem', 'oratsionem', 'orattionem', 'oratuionem', 'oratvionem', 'oratwionem', 'oratxionem', 'oratyionem', 'oratzionem', 'oratiaonem', 'oratibonem', 'oraticonem', 'oratidonem', 'oratieonem', 'oratifonem', 'oratigonem', 'oratihonem', 'oratiionem', 'oratijonem', 'oratikonem', 'oratilonem', 'oratimonem', 'oratinonem', 'oratioonem', 'oratiponem', 'oratiqonem', 'oratironem', 'oratisonem', 'oratitonem', 'oratiuonem', 'orativonem', 'oratiwonem', 'oratixonem', 'oratiyonem', 'oratizonem', 'oratioanem', 'oratiobnem', 'oratiocnem', 'oratiodnem', 'oratioenem', 'oratiofnem', 'oratiognem', 'oratiohnem', 'oratioinem', 'oratiojnem', 'oratioknem', 'oratiolnem', 'oratiomnem', 'orationnem', 'oratioonem', 'oratiopnem', 'oratioqnem', 'oratiornem', 'oratiosnem', 'oratiotnem', 'oratiounem', 'oratiovnem', 'oratiownem', 'oratioxnem', 'oratioynem', 'oratioznem', 'orationaem', 'orationbem', 'orationcem', 'orationdem', 'orationeem', 'orationfem', 'orationgem', 'orationhem', 'orationiem', 'orationjem', 'orationkem', 'orationlem', 'orationmem', 'orationnem', 'orationoem', 'orationpem', 'orationqem', 'orationrem', 'orationsem', 'orationtem', 'orationuem', 'orationvem', 'orationwem', 'orationxem', 'orationyem', 'orationzem', 'orationeam', 'orationebm', 'orationecm', 'orationedm', 'orationeem', 'orationefm', 'orationegm', 'orationehm', 'orationeim', 'orationejm', 'orationekm', 'orationelm', 'orationemm', 'orationenm', 'orationeom', 'orationepm', 'orationeqm', 'orationerm', 'orationesm', 'orationetm', 'orationeum', 'orationevm', 'orationewm', 'orationexm', 'orationeym', 'orationezm', 'orationema', 'orationemb', 'orationemc', 'orationemd', 'orationeme', 'orationemf', 'orationemg', 'orationemh', 'orationemi', 'orationemj', 'orationemk', 'orationeml', 'orationemm', 'orationemn', 'orationemo', 'orationemp', 'orationemq', 'orationemr', 'orationems', 'orationemt', 'orationemu', 'orationemv', 'orationemw', 'orationemx', 'orationemy', 'orationemz']\n"
     ]
    }
   ],
   "source": [
    "def insert_letter(word):\n",
    "    split_l = []\n",
    "    insert_list = []\n",
    "    for i in range(len(word) + 1):\n",
    "        split_l.append((word[0:i], word[i:]))\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_list = [a + l + b for a, b in split_l for l in letters]\n",
    "    # print(split_l)\n",
    "    return insert_list\n",
    "print(insert_letter(\"orationem\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 0: plane, probability 0.001825\n",
      "word 1: sint, probability 0.001825\n",
      "word 2: se, probability 0.005474\n",
      "word 3: sunt, probability 0.005474\n",
      "word 4: bene, probability 0.001825\n",
      "word 5: ne, probability 0.005474\n",
      "word 6: an, probability 0.003650\n",
      "word 7: sese, probability 0.001825\n",
      "word 8: ante, probability 0.001825\n",
      "word 9: mene, probability 0.001825\n",
      "word 10: suae, probability 0.001825\n"
     ]
    }
   ],
   "source": [
    "def edit_one_letter(word, allow_switches=True):\n",
    "    edit_set1 = set()\n",
    "    edit_set1.update(DeleteLetter(word))\n",
    "    if allow_switches:\n",
    "        edit_set1.update(SwitchLetter(word))\n",
    "    edit_set1.update(replace_letter(word))\n",
    "    edit_set1.update(insert_letter(word))\n",
    "    return edit_set1\n",
    "\n",
    "\n",
    "print(\"word 0: plane, probability 0.001825\")\n",
    "print(\"word 1: sint, probability 0.001825\")\n",
    "print(\"word 2: se, probability 0.005474\")\n",
    "print(\"word 3: sunt, probability 0.005474\")\n",
    "print(\"word 4: bene, probability 0.001825\")\n",
    "print(\"word 5: ne, probability 0.005474\")\n",
    "print(\"word 6: an, probability 0.003650\")\n",
    "print(\"word 7: sese, probability 0.001825\")\n",
    "print(\"word 8: ante, probability 0.001825\")\n",
    "print(\"word 9: mene, probability 0.001825\")\n",
    "print(\"word 10: suae, probability 0.001825\")\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "# edit two letters\n",
    "def edit_two_letters(word, allow_switches=True):\n",
    "    edit_set2 = set()\n",
    "    edit_one = edit_one_letter(word, allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letter(w, allow_switches=allow_switches)\n",
    "            edit_set2.update(edit_two)\n",
    "    return edit_set2\n",
    "\n",
    "\n",
    "\n",
    "# get corrected word\n",
    "def get_corrections(word, probs, vocab, n=2):\n",
    "    suggested_word = []\n",
    "    best_suggestion = []\n",
    "    suggested_word = list(\n",
    "        (word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(\n",
    "            vocab))\n",
    "    best_suggestion = [[s, probs[s]] for s in list(reversed(suggested_word))]\n",
    "    return best_suggestion\n",
    "\n",
    "\n",
    "\n",
    "my_word = input(\"Enter any word:\")\n",
    "probs = get_probs(word_count_dict)\n",
    "tmp_corrections = get_corrections(my_word, probs, v, 2)\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9d0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
